# CountVectorizer Comparison Project

Проект для сравнения различных методов предобработки текста при использовании CountVectorizer в машинном обучении.

## Описание проекта

Этот проект сравнивает 5 различных подходов к векторизации текста с использованием `CountVectorizer` из библиотеки scikit-learn:
1. **Базовый метод** - стандартный CountVectorizer без дополнительной обработки
2. **С удалением стоп-слов** - удаление общеупотребительных слов (например, "the", "and", "is")
3. **С лемматизацией** - приведение слов к их нормальной форме (лемме)
4. **Со стеммингом** - обрезание окончаний слов до основы (стема)
5. **С простым токенизатором** - быстрое разделение по пробелам без обработки пунктуации

Проект использует датасет новостей BBC для классификации и сравнивает эффективность каждого метода по нескольким метрикам.

## Быстрый старт

### Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/IlyaShaposhnikov/count-vectorizer.git
cd count-vectorizer
```

2. Установите зависимости:

```bash
pip install -r requirements.txt
```

### Использование
Запустите основной скрипт сравнения:

```bash
python main.py
```

Скрипт автоматически:
1. Загрузит необходимые ресурсы NLTK
2. Загрузит и подготовит данные
3. Обучит модели с разными методами векторизации
4. Выведет сравнение результатов
5. Создаст визуализации

## Структура проекта
```text
count-vectorizer/
├── data/                               # Папка с данными
│   └── bbc_text_cls.csv                # Датасет новостей BBC
├── methods/                            # Реализации различных методов векторизации
│   ├── base_vectorizer.py              # Базовый метод
│   ├── stopwords_vectorizer.py         # С удалением стоп-слов
│   ├── lemmatization_vectorizer.py     # С лемматизацией
│   ├── stemming_vectorizer.py          # Со стеммингом
│   └── simple_tokenizer_vectorizer.py  # С простым токенизатором
├── utils/                              # Вспомогательные утилиты
│   └── nltk_utils.py                   # Утилиты для работы с NLTK
├── results/                            # Папка для сохранения результатов
├── main.py                             # Основной скрипт сравнения
├── requirements.txt                    # Зависимости проекта
└── README.md                           # Этот файл
```

## Метрики сравнения
Каждый метод оценивается по следующим метрикам:
* Точность на тренировочных данных - точность модели на данных, на которых она обучалась
* Точность на тестовых данных - точность модели на новых данных (основная метрика)
* Размер словаря - количество уникальных слов после обработки
* Плотность матрицы - процент ненулевых элементов в матрице признаков
* Время выполнения - общее время обучения и оценки метода

## Технические детали
### Используемые библиотеки
* numpy - работа с числовыми массивами
* pandas - обработка и анализ данных
* scikit-learn - машинное обучение и CountVectorizer
* nltk - обработка естественного языка
* matplotlib/seaborn - визуализация результатов
* tabulate - красивое отображение таблиц

### Датасет
Проект использует датасет BBC News, содержащий 2225 документов в 5 категориях:
* business (510 документов)
* entertainment (386 документов)
* politics (417 документов)
* sport (511 документов)
* tech (401 документ)

### Алгоритм классификации
Для всех методов используется Multinomial Naive Bayes - алгоритм, хорошо зарекомендовавший себя для классификации текстов.

## Пример вывода
После запуска `main.py` вы увидите:
1. Прогресс выполнения каждого метода
2. Детальную таблицу сравнения
3. Графики с визуализацией результатов
4. Сохраненные результаты в папке results/

## Ключевые выводы
Из сравнения обычно следует:
1. Удаление стоп-слов уменьшает размерность и часто улучшает качество
2. Лемматизация дает более качественные результаты, чем стемминг
3. Простые методы быстрее, но могут быть менее точными
4. Размер словаря напрямую влияет на время обучения модели
5. Плотность матрицы обычно очень низкая (<1%) для текстовых данных

## Лицензия
Этот проект распространяется под лицензией MIT. 

## Автор
Илья Шапошников
